{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbop9uxB6nzU",
        "outputId": "063ee0d2-6709-44df-dd62-c2a1ccc3fbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mColab Secrets'tan API Anahtarları Yükleniyor...\n",
            "-> Gemini API Key (LLM için) yüklendi.\n",
            "-> Kaggle API Kimlik Bilgileri yüklendi.\n"
          ]
        }
      ],
      "source": [
        "# 1. GEREKLİ KÜTÜPHANE KURULUMLARI VE ORTAM AYARLARI\n",
        "\n",
        "# Bu adımda RAG mimarisi için gerekli olan tüm kütüphaneler kurulmaktadır.\n",
        "!pip install google-genai langchain langchain-google-genai langchain-community faiss-cpu opendatasets pandas gradio sentence-transformers -q\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import glob\n",
        "\n",
        "# RAG Bileşenleri Import'ları\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "import gradio as gr\n",
        "\n",
        "# Global Değişken Tanımları\n",
        "KAGGLE_DATASET_URL = \"https://www.kaggle.com/datasets/cuddlefish/fairy-tales\"\n",
        "TEXT_FILE_PATH = \"initial_placeholder.txt\"\n",
        "FAISS_DB_PATH = \"faiss_index_fairy_tales\"\n",
        "\n",
        "# --- API Anahtarlarını Colab Secrets'tan Yükleme ---\n",
        "print(\"Colab Secrets'tan API Anahtarları Yükleniyor...\")\n",
        "\n",
        "# GEMINI API Anahtarını Yükleme (LLM için)\n",
        "GEMINI_API_KEY_VALUE = None\n",
        "if not os.getenv(\"GEMINI_API_KEY\"):\n",
        "    try:\n",
        "        GEMINI_API_KEY_VALUE = userdata.get('GEMINI_API_KEY')\n",
        "        os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY_VALUE\n",
        "        print(\"-> Gemini API Key (LLM için) yüklendi.\")\n",
        "    except Exception:\n",
        "        print(\"HATA: 'GEMINI_API_KEY' Colab Secrets'ta bulunamadı.\")\n",
        "else:\n",
        "    GEMINI_API_KEY_VALUE = os.getenv(\"GEMINI_API_KEY\")\n",
        "    print(\"-> Gemini API Key (LLM için) ortam değişkeninden alındı.\")\n",
        "\n",
        "# KAGGLE API Kimlik Bilgilerini Yükleme\n",
        "if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "    try:\n",
        "        os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "        os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "        print(\"-> Kaggle API Kimlik Bilgileri yüklendi.\")\n",
        "    except Exception:\n",
        "        print(\"UYARI: Kaggle kimlik bilgileri Secrets'ta bulunamadı.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. VERİ SETİ EDİNİMİ VE METİN YOLU DÜZELTMESİ\n",
        "\n",
        "# Veri setini Kaggle CLI ile indirip unzip ediyoruz.\n",
        "!kaggle datasets download -d cuddlefish/fairy-tales --unzip\n",
        "print(\"--- İndirme Tamamlandı ---\")\n",
        "\n",
        "# --- İndirilen Dosyanın Yolunu Otomatik Bulma ve Güncelleme ---\n",
        "\n",
        "def find_and_update_path():\n",
        "    data_file_path = None\n",
        "\n",
        "    # Doğrudan /content (ana) dizinindeki ve alt klasörlerdeki tüm .txt dosyalarını ara\n",
        "    txt_files = glob.glob('*.txt')\n",
        "    for name in os.listdir('.'):\n",
        "        if os.path.isdir(name) and ('fairy' in name.lower() or 'tales' in name.lower()):\n",
        "            txt_files.extend(glob.glob(os.path.join(name, '*.txt')))\n",
        "\n",
        "    if txt_files:\n",
        "        # En büyük dosyayı seç\n",
        "        txt_files.sort(key=lambda x: os.path.getsize(x), reverse=True)\n",
        "        data_file_path = txt_files[0]\n",
        "\n",
        "        # TEXT_FILE_PATH değişkenini güncelle\n",
        "        globals()['TEXT_FILE_PATH'] = data_file_path\n",
        "\n",
        "        print(\"\\n--- TEXT_FILE_PATH BAŞARIYLA GÜNCELLENDİ ---\")\n",
        "        print(\"Yeni yol:\", globals()['TEXT_FILE_PATH'])\n",
        "        return True\n",
        "    else:\n",
        "        print(\"HATA: Ana dizinde veya alt klasörlerde .txt uzantılı dosya bulunamadı.\")\n",
        "        return False\n",
        "\n",
        "# Fonksiyonu Çalıştırma\n",
        "find_and_update_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa8KVQizOtjD",
        "outputId": "e67cc6f0-08c6-43b6-c678-d10197945c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/cuddlefish/fairy-tales\n",
            "License(s): CC0-1.0\n",
            "Downloading fairy-tales.zip to /content\n",
            "  0% 0.00/4.44M [00:00<?, ?B/s]\n",
            "100% 4.44M/4.44M [00:00<00:00, 301MB/s]\n",
            "--- İndirme Tamamlandı ---\n",
            "\n",
            "--- TEXT_FILE_PATH BAŞARIYLA GÜNCELLENDİ ---\n",
            "Yeni yol: merged_clean.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. VERİ PARÇALAMA (CHUNKING) VE HIZ MODU LİMİTİ\n",
        "\n",
        "# KRİTİK AYAR: Bu değişkeni False yaparak hızlı teslimat için 1000 parça limiti etkinleştirilir.\n",
        "FULL_RUN_MODE = False\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Tüm veri setini okur, parçalar ve FULL_RUN_MODE'a göre limitler.\"\"\"\n",
        "    mode_text = \"Tüm Koleksiyon (Yavaş)\" if FULL_RUN_MODE else \"Limitli Koleksiyon (Hızlı)\"\n",
        "    print(f\"\\n--- Veri Seti Hazırlanıyor ({mode_text}) ---\")\n",
        "\n",
        "    # Metin dosyasını okuma\n",
        "    try:\n",
        "        file_path = globals().get('TEXT_FILE_PATH')\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            full_text = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Hata: Metin dosyası bulunamadı: {globals().get('TEXT_FILE_PATH')}\")\n",
        "        return []\n",
        "\n",
        "    # Basit RecursiveCharacterTextSplitter ayarları\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    all_texts = text_splitter.split_text(full_text)\n",
        "    texts = all_texts\n",
        "\n",
        "    if not FULL_RUN_MODE:\n",
        "        MAX_DOCUMENTS = 1000\n",
        "        texts = all_texts[:MAX_DOCUMENTS]\n",
        "        print(f\"UYARI: Proje hızlandırması için parça sayısı {MAX_DOCUMENTS} ile sınırlandırıldı.\")\n",
        "\n",
        "    documents = [Document(page_content=chunk.strip()) for chunk in texts]\n",
        "\n",
        "    print(f\"Toplam {len(documents)} adet parçalanmış doküman oluşturuldu.\")\n",
        "    return documents\n",
        "\n",
        "# Fonksiyonu Çalıştırma\n",
        "documents = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMUmndtlOwjT",
        "outputId": "62e5430f-333b-48c1-eb03-ea19871b0e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Veri Seti Hazırlanıyor (Limitli Koleksiyon (Hızlı)) ---\n",
            "UYARI: Proje hızlandırması için parça sayısı 1000 ile sınırlandırıldı.\n",
            "Toplam 1000 adet parçalanmış doküman oluşturuldu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. EMBEDDING VE FAISS İNDEKS OLUŞTURMA (T4 GPU OPTİMİZASYONU)\n",
        "\n",
        "import shutil # Eski indeksi silmek için\n",
        "\n",
        "def create_vector_store(documents):\n",
        "    \"\"\"Dokümanları embed eder ve FAISS Vektör Veri Tabanı oluşturur/yükler.\"\"\"\n",
        "    print(\"\\n--- FAISS Vektör Veri Tabanı Hazırlanıyor ---\")\n",
        "\n",
        "    # Embedding Modeli: all-mpnet-base-v2\n",
        "    EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
        "    print(f\"Embedding modeli olarak HuggingFace '{EMBEDDING_MODEL_NAME}' kullanılıyor...\")\n",
        "\n",
        "    # KRİTİK AYAR: Modeli zorla T4 GPU'ya (cuda) yükleme.\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        model_kwargs={'device': 'cuda'}\n",
        "    )\n",
        "\n",
        "    # Eski FAISS indeksini silme\n",
        "    if os.path.exists(FAISS_DB_PATH):\n",
        "        print(f\"UYARI: Yeni veri boyutu kullanıldığı için eski FAISS indeksi ({FAISS_DB_PATH}) SİLİNİYOR.\")\n",
        "        shutil.rmtree(FAISS_DB_PATH)\n",
        "\n",
        "\n",
        "    print(\"FAISS indeksi oluşturuluyor ve kaydediliyor...\")\n",
        "    if not documents:\n",
        "        print(\"Hata: İşlenecek doküman bulunmadığı için indeks oluşturulamadı.\")\n",
        "        return None\n",
        "\n",
        "    # Dokümanları embed et ve FAISS'e ekle\n",
        "    vector_store = FAISS.from_documents(documents, embedding_model)\n",
        "\n",
        "    # İndeksi kaydet\n",
        "    vector_store.save_local(FAISS_DB_PATH)\n",
        "    print(f\"FAISS indeksi {FAISS_DB_PATH} konumuna başarıyla kaydedildi.\")\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "# Fonksiyonu Çalıştırma\n",
        "vector_store = create_vector_store(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "68bdb9d89fa746e1874ebce89e53480d",
            "be9c07f90eed4c55b35fa531f49a335c",
            "12cd1582601e4d388ce63774d03aabc3",
            "cbe77c6dbc36486e9b1b7885bfe915b7",
            "ba0d154145134bc5b92a91f6c85c22ac",
            "becd19c4df6b4dcaa146387b31cc13b5",
            "497f1bc29f6745a4bb996652977d894a",
            "fa6efe0d46884aaa97fa80e98259caaa",
            "bf911c685bc64a6986712a44ffa0f194",
            "ce2103be8fd74946855355eb3681e6fe",
            "f4d2f3c505874a69b53679eb14b48be1",
            "233dd70c6b1541ca97e53a3a834c9f90",
            "5f30cb7c4e274ba086e3ad35186b6a48",
            "1acaf75461944fd4a73ffe2a181f0407",
            "8be6e4ea8ccd4482ad0505d8f652b7ca",
            "61093c6e70934be4bca656c07b717ff8",
            "d129fc609578469b8a9a28ff17150780",
            "36030f29a78c4a7a96ba5b6fd2e0a051",
            "89b74a9e54b0402c8b1fa30901a282a2",
            "fea2d74890fa4434b859d42ace133b16",
            "a49647dfbfff465a8142ef2f7253cb54",
            "4bede36951d7459ea098eec0af5dcf41",
            "3fcec392ca5f436fafa4296bb57a8446",
            "fc95ea57317e4ca1be1c1c530ae530b7",
            "03ed1dc567a5495eb3ac3ac0d4562050",
            "f9031c37341a4bc2addf941b67c303ab",
            "8a1fe9f870f448c3888e979164afd08a",
            "1d1db87339d143aba3adac8e6156c26a",
            "4ea5edd1c9ef4c77a8fa07669b741b64",
            "a867c35c8dc3455cb1e5315ce38c86c5",
            "415d2a843c5044d899b7cffac113adca",
            "78ce1e9fe18d46369ac70cb6d9456264",
            "37bba274957c48f5ba0bda65fd636ee6",
            "a00d0cc65ae3448390b34012d97f55f6",
            "23d4fe9d38e34c5f87413e4bc2859ddc",
            "cbf167ee3e94448194ee3a785e41f7a2",
            "8d014e79eb204a8ab0ac0d3994e7d63b",
            "faab56ca27d742e387311a29bb95e42d",
            "0b666fa7b9fe4faeb35786d5d420b86d",
            "e1afc7ffdfff47148297ed76085cf9b6",
            "536f4face50c4d0a88a3b2b08eb9588e",
            "37d94f2f94a040f19ed26e61d284b9c7",
            "618122e7899f4e39923c3ad0644b3d98",
            "5d5a363a2ea34682ba6f9012ca912aee",
            "21808989b4b24fa7aa963c092b0a35f0",
            "07ada489aeac470c9a198441f5da91d3",
            "9c3c8e58afaf4b339b34c5ef41a8700b",
            "ebc1cdd414d8458cbbda60dda090c56a",
            "8d387950bda644bc823455d5e3ce8c79",
            "bda17ba26b1d496c8d07e8a3ab1ce4a1",
            "6c751779ef2f48d1b1cc158ab8db260c",
            "dbc654358d674fab98603ed5d96ce3ad",
            "e0b27198ae624a3496d4294967f0005d",
            "07533b9c90c94bef8345ae2ca368f271",
            "be2e2bbac2114647be9f94ea9fe4d502",
            "b000484e3e414f4a9c4f7a0ab23cacca",
            "3732af1eb0814308910dfc1d50ee9a30",
            "98a938a97746470bb0f023c1720261bb",
            "592c9f63e70547089ebbdd72aa3a849e",
            "b64a30d4b1b342a8906d96cca674d8bd",
            "2c869dd2ab05402d93c58660c9b13587",
            "48431d4787894d98be6f89cd6e7f2649",
            "28f7a9c3497a42aa89026210f32f72cd",
            "c16d39716ed4468a98eeeb33ce20f15d",
            "ad59d61c2cf44c27a628990bc2ab26a1",
            "cd136944855c44a68f22d038a9a762d5",
            "d30267be94d0415082b04b3dde4af1a1",
            "58ee0ba56cba4d37af09f85da79046f9",
            "f7d88e19dd6445c2a5b6740689e53e8a",
            "28cfa3a19fd34050bf378eb272ab0eb0",
            "abc67c433fa54a2a988af03dfa1f64c1",
            "b17a05fe31be4baab13671544a6438f8",
            "794226b7998443d5ba0714309a9f001a",
            "df95224b7e5a420380f0a60a6293cba0",
            "227bb53bd9c1458b90120347e234e72f",
            "1761f5e38ba8423e8570be6ce015e4ad",
            "fdcff2ec484747b8ae99a92be42c0894",
            "919d98e5961e4482a7b771f43f422a35",
            "c6c18c49e613496ea59f99d580f1512d",
            "f659bf7f0af34e78a9aa566f5db4216a",
            "119f0e0b3f41450ea05f32b076dbb65f",
            "79ca48514e864072b53cbc62c5c4757e",
            "b38274993e32413a86661c7d2692ea6d",
            "d1936220acfc4fa1b926b368abccbf67",
            "896b9da40c624ce5a4fd8963e966583c",
            "963b7bdda18242599bffd2d37fc83f7e",
            "67fab3b19e6e4d90a6453286687eb475",
            "d0975c0f3c6744dd962db24c2a737466",
            "4275195287ea4482925d2f5ae8eb8dce",
            "b885e93917c04ceab1f292f6ee9d57d0",
            "0684bf9199e8490db356a8d07015c77d",
            "2365423599734a3db99052849fae700c",
            "3f31c0b4bb334f218f8e54a361699e2e",
            "13aa8d28e9d94204869518786cb6a1f0",
            "2dd3daba37a84e399d482fc2f7879394",
            "6015f2fc2e2647fbb295863b309ce2f4",
            "8a10b5dfda4246f19988937198b3f22c",
            "ad794bef36774238a053b1cdc3e207f6",
            "d16a00069235445daf0f43e2cd262368",
            "b31384aff0034eaab5fe65e632305942",
            "4b6f70496bd44bad99b067e0f1f3d73b",
            "1d6156afeb8a43c5b1b1c1510e01afe0",
            "6cf25aca61f84f50a81ec49202de2492",
            "04374094d340424fbc6df551222601be",
            "467561ca33724ef9ba5e2b14e8e5c57c",
            "cb96fe42094c45cb812bd4f460b8e089",
            "0fa7f15dce0849c886ba16a2a411bb8c",
            "82bd23c677d14dcbb3bd53203d549eb5",
            "33464c08e3454388ab164af4b4200e96",
            "bb9807f288a9417f9556beb1f3eb9516",
            "a3f7f6f8b265416b817b06e5a677aff0",
            "6ce9741ead784da78890674d517afae1",
            "25548256b2524de2af6c3c335cde5b37",
            "2ddf574cf25a46d8bcc050db3d198150",
            "05e3b75b104a48eda2518517a20646cc",
            "f2de5326917b4484ba7fc70154e8e118",
            "b8a56e98ecb74f4a9aee2ae1b3855ff3",
            "f79e6a884d3f481d919fbd93f0e369bb",
            "ad0046b7ddbe49f0b29df5258a4badaa",
            "bc6fe0eddc1f4c8ea80ec97264a79dc9",
            "f4d5c5fab89e4a1bae5f0f5d246450d5"
          ]
        },
        "id": "vNzl6XU1OxhK",
        "outputId": "debc0bf6-13c9-41e4-e68a-1a680e1d2521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FAISS Vektör Veri Tabanı Hazırlanıyor ---\n",
            "Embedding modeli olarak HuggingFace 'all-mpnet-base-v2' kullanılıyor...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1652042957.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68bdb9d89fa746e1874ebce89e53480d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "233dd70c6b1541ca97e53a3a834c9f90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fcec392ca5f436fafa4296bb57a8446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a00d0cc65ae3448390b34012d97f55f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21808989b4b24fa7aa963c092b0a35f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b000484e3e414f4a9c4f7a0ab23cacca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d30267be94d0415082b04b3dde4af1a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "919d98e5961e4482a7b771f43f422a35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4275195287ea4482925d2f5ae8eb8dce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b31384aff0034eaab5fe65e632305942"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3f7f6f8b265416b817b06e5a677aff0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS indeksi oluşturuluyor ve kaydediliyor...\n",
            "FAISS indeksi faiss_index_fairy_tales konumuna başarıyla kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. RAG ZİNCİRİ KURULUMU VE AKADEMİK PROMPT MÜHENDİSLİĞİ\n",
        "\n",
        "def setup_rag_chain(vector_store):\n",
        "    \"\"\"RAG zincirini (Chain) ayarlar.\"\"\"\n",
        "    print(\"\\n--- RAG Zinciri Kuruluyor (Gemini 2.0 Flash) ---\")\n",
        "\n",
        "    if not vector_store or not globals().get('GEMINI_API_KEY_VALUE'):\n",
        "        print(\"Hata: Vektör veritabanı veya Gemini API Anahtarı mevcut değil. Zincir oluşturulamadı.\")\n",
        "        return None\n",
        "\n",
        "    llm = GoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        google_api_key=globals().get('GEMINI_API_KEY_VALUE')\n",
        "    )\n",
        "\n",
        "    # Retriever: Arama derinliğini k=8'e çıkarıyoruz.\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 8})\n",
        "\n",
        "    # PROMPT GÜNCELLEMESİ: AKADEMİK TON VE SIKI RAG KURALI\n",
        "    template = \"\"\"You are an AI research assistant specializing in literary analysis of fairy tales.\n",
        "    Your task is to answer the user's question **STRICTLY** using the provided context.\n",
        "    If the answer is not present in the context, politely respond with: 'I am sorry, I could not find this information in the fairy tales available to me.'\n",
        "    Formulate your answer using an **objective and academic tone** suitable for a literary scholar. Avoid flowery or overly creative language.\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    # RetrievalQA Zinciri\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False, # DEBUG kapalı.\n",
        "        chain_type_kwargs={\"prompt\": custom_rag_prompt}\n",
        "    )\n",
        "\n",
        "    print(\"RAG zinciri başarıyla kuruldu. Akademik ton (k=8) etkin.\")\n",
        "    return qa_chain\n",
        "\n",
        "def answer_question(qa_chain, question):\n",
        "    \"\"\"RAG zincirini kullanarak bir soruyu cevaplar.\"\"\"\n",
        "    if not qa_chain:\n",
        "        return \"Chatbot failed to initialize. Please check previous steps.\"\n",
        "\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": question})\n",
        "        answer = result['result']\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error during Gemini/LLM call: {e}\"\n",
        "        return \"I am sorry, a problem occurred. Please try again later.\"\n",
        "\n",
        "# Fonksiyonu Çalıştırma\n",
        "rag_chain = setup_rag_chain(vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgCrTKvwO0Gc",
        "outputId": "85890e97-c573-4922-9b74-0d9c220830d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- RAG Zinciri Kuruluyor (Gemini 2.0 Flash) ---\n",
            "RAG zinciri başarıyla kuruldu. Akademik ton (k=8) etkin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. CHATBOT ARAYÜZÜNÜ BAŞLATMA (GRADIO)\n",
        "\n",
        "def launch_gradio_app(qa_chain):\n",
        "    \"\"\"Gradio arayüzünü başlatır.\"\"\"\n",
        "\n",
        "    if qa_chain is None:\n",
        "        def dummy_predict(message, history):\n",
        "            return \"Chatbot failed to initialize. Please check previous steps.\"\n",
        "        predict_func = dummy_predict\n",
        "        title = \"Masal Chatbotu (Hata: Başlatılamadı)\"\n",
        "    else:\n",
        "        def predict(message, history):\n",
        "            return answer_question(qa_chain, message)\n",
        "\n",
        "        predict_func = predict\n",
        "        title = \"Tale Me A Story - RAG Tabanlı Masal Chatbotu\"\n",
        "\n",
        "    print(\"\\n--- Gradio Arayüzü Başlatılıyor ---\")\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=predict_func,\n",
        "        title=title,\n",
        "        description=\"Akademik bir tonda yanıt veren RAG destekli masal analiz asistanı. Lütfen sorlarınızı İngilizce sorun.\"\n",
        "    ).launch(share=True)\n",
        "\n",
        "# Fonksiyonu Çalıştırma\n",
        "launch_gradio_app(rag_chain)"
      ],
      "metadata": {
        "id": "AVl8sSVbO4va",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "cba050c8-6e9a-43bd-c822-d3669fdcdb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Gradio Arayüzü Başlatılıyor ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8b1009007dc18bc0e7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8b1009007dc18bc0e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dUovv5pD1qGl"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
},
  "nbformat": 4,
  "nbformat_minor": 0
}
